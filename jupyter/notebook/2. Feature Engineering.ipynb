{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#1F618D\"> Newyork City Job Postings - Data Engineering Challenge </font>\n",
    "### <font color=\"#F5B041\"> Part-2: Feature Engineering </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    concat_ws,\n",
    "    regexp_replace,\n",
    "    split,\n",
    "    lit,\n",
    "    lower,\n",
    "    explode,\n",
    "    count,\n",
    ")\n",
    "import re\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    regexp_replace,\n",
    "    lower,\n",
    "    split,\n",
    "    udf,\n",
    "    avg,\n",
    "    explode,\n",
    "    count,\n",
    ")\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 0] Error>\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize NLTK lemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"NYCJobsFeatureEngineering\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    \"../../dataset/raw_data/nyc-jobs.csv\", header=True, inferSchema=True, escape='\"'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#1F618D\"> 1. Feature Extraction - </font> <font color=\"#F5B041\"> Annual Salary </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_annual_salary(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize salary range columns to annual salary.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame containing salary information.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with standardized annual salary columns.\n",
    "    \"\"\"\n",
    "    workhours_per_day = 8\n",
    "    workdays_per_week = 5\n",
    "    workweeks_per_year = 52\n",
    "\n",
    "    workdays_per_year = workdays_per_week * workweeks_per_year\n",
    "    workhours_per_year = workhours_per_day * workdays_per_year\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"AnnualSalaryFrom\",\n",
    "        when(\n",
    "            col(\"Salary Frequency\") == \"Daily\",\n",
    "            col(\"Salary Range From\") * workdays_per_year,\n",
    "        )\n",
    "        .when(\n",
    "            col(\"Salary Frequency\") == \"Hourly\",\n",
    "            col(\"Salary Range From\") * workhours_per_year,\n",
    "        )\n",
    "        .otherwise(col(\"Salary Range From\")),\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"AnnualSalaryTo\",\n",
    "        when(\n",
    "            col(\"Salary Frequency\") == \"Daily\",\n",
    "            col(\"Salary Range To\") * workdays_per_year,\n",
    "        )\n",
    "        .when(\n",
    "            col(\"Salary Frequency\") == \"Hourly\",\n",
    "            col(\"Salary Range To\") * workhours_per_year,\n",
    "        )\n",
    "        .otherwise(col(\"Salary Range To\")),\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = standardize_annual_salary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting average annual salary feature\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"AverageAnnualSalary\", (col(\"AnnualSalaryFrom\") + col(\"AnnualSalaryTo\")) / 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#1F618D\"> 2. Feature Extraction - </font>  <font color=\"#F5B041\"> Degree List & Highest Degree </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords for degrees\n",
    "keywords = [\n",
    "    \"master\",\n",
    "    \"phd\",\n",
    "    \"pg\",\n",
    "    \"post graduate\",\n",
    "    \"baccalaureate\",\n",
    "    \"diploma\",\n",
    "    \"high school\",\n",
    "]\n",
    "\n",
    "# Define the priority order for degrees\n",
    "degree_priority = [\n",
    "    \"phd\",\n",
    "    \"master\",\n",
    "    \"post graduate\",\n",
    "    \"pg\",\n",
    "    \"baccalaureate\",\n",
    "    \"diploma\",\n",
    "    \"high school\",\n",
    "]\n",
    "\n",
    "\n",
    "def build_regex(keywords: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Build a regular expression pattern to match keywords in a line.\n",
    "\n",
    "    Parameters:\n",
    "        keywords (list of str): List of keywords to build the regex pattern for.\n",
    "\n",
    "    Returns:\n",
    "        str: The regex pattern to match the keywords.\n",
    "    \"\"\"\n",
    "    res = \"(\"\n",
    "    for key in keywords:\n",
    "        res += \"\\\\b\" + key + \"\\\\b|\"\n",
    "    res = res[0 : len(res) - 1] + \")\"\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_matching_string(line: str, regex: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all matches of a regex pattern in a line.\n",
    "\n",
    "    Parameters:\n",
    "        line (str): The input line to search for matches.\n",
    "        regex (str): The regex pattern to search for.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of matching strings found in the line.\n",
    "    \"\"\"\n",
    "    matches = re.findall(regex, line)\n",
    "    return matches if matches else []\n",
    "\n",
    "\n",
    "def get_highest_degree(degrees: List[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get the highest priority degree from a list of degrees.\n",
    "\n",
    "    Parameters:\n",
    "        degrees (list of str): List of degrees to choose from.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The highest priority degree or None if no valid degree found.\n",
    "    \"\"\"\n",
    "\n",
    "    if degrees:\n",
    "        for degree in degree_priority:\n",
    "            if degree in degrees:\n",
    "                return degree\n",
    "    return None\n",
    "\n",
    "\n",
    "def safe_get_matching_string(line, regex):\n",
    "    if isinstance(line, (str, bytes)):\n",
    "        return get_matching_string(line, regex)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "get_degree_list_udf = udf(\n",
    "    lambda line, regex: safe_get_matching_string(line, regex), ArrayType(StringType())\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"degrees\",\n",
    "    get_degree_list_udf(df[\"Minimum Qual Requirements\"], lit(build_regex(keywords))),\n",
    ")\n",
    "\n",
    "get_highest_degree_udf = udf(get_highest_degree, StringType())\n",
    "\n",
    "df = df.withColumn(\"HighestDegree\", get_highest_degree_udf(col(\"degrees\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------+\n",
      "|degrees                              |HighestDegree|\n",
      "+-------------------------------------+-------------+\n",
      "|[baccalaureate]                      |baccalaureate|\n",
      "|[baccalaureate, high school, diploma]|baccalaureate|\n",
      "|[high school]                        |high school  |\n",
      "|[high school]                        |high school  |\n",
      "+-------------------------------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"degrees\", \"HighestDegree\").show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"#1F618D\"> 3. Feature Extraction - </font>  <font color=\"#F5B041\"> Skills as ngrams</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF for lemmatization using NLTK\n",
    "def lemmatize_words(words):\n",
    "    if words is not None:\n",
    "        return [lemmatizer.lemmatize(word) for word in words]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df.withColumn(\n",
    "    \"cleaned_text\", lower(regexp_replace(col(\"Preferred SKills\"), \"[^a-zA-Z\\s]\", \"\"))\n",
    ")\n",
    "# Display the tokenized words\n",
    "tokenized_df = cleaned_df.withColumn(\"words\", split(col(\"cleaned_text\"), \"\\s+\"))\n",
    "\n",
    "# Remove stop words using StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_tokenized_df = tokenized_df.filter(col(\"words\").isNotNull())\n",
    "filtered_df = remover.transform(filtered_tokenized_df)\n",
    "\n",
    "# Create the UDF\n",
    "lemmatize_udf = udf(lemmatize_words, ArrayType(StringType()))\n",
    "\n",
    "# Apply lemmatization using the UDF\n",
    "lemmatized_df = filtered_df.withColumn(\"lemmatized_words\", lemmatize_udf(col(\"words\")))\n",
    "\n",
    "# Extract n-grams\n",
    "ngram = NGram(n=1, inputCol=\"lemmatized_words\", outputCol=\"ngrams\")\n",
    "ngram_df = ngram.transform(lemmatized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all the columns which are not required for kpi analysis\n",
    "\n",
    "cols_to_drop = [\n",
    "    \"Civil Service Title\",\n",
    "    \"Title Code No\",\n",
    "    \"Level\",\n",
    "    \"Full-Time/Part-Time indicator\",\n",
    "    \"Salary Range From\",\n",
    "    \"Salary Range To\",\n",
    "    \"Salary Frequency\",\n",
    "    \"Work Location\",\n",
    "    \"Division/Work Unit\",\n",
    "    \"Job Description\",\n",
    "    \"Minimum Qual Requirements\",\n",
    "    \"Preferred Skills\",\n",
    "    \"Additional Information\",\n",
    "    \"To Apply\",\n",
    "    \"Hours/Shift\",\n",
    "    \"Work Location 1\",\n",
    "    \"Recruitment Contact\",\n",
    "    \"Residency Requirement\",\n",
    "    \"Post Until\",\n",
    "    \"Posting Updated\",\n",
    "    \"Process Date\",\n",
    "    \"cleaned_text\",\n",
    "    \"words\",\n",
    "    \"filtered_words\",\n",
    "    \"lemmatized_words\",\n",
    "]\n",
    "\n",
    "ngram_df = ngram_df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory path\n",
    "target_directory = \"../../dataset/processed/\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(target_directory):\n",
    "    os.makedirs(target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns with invalid characters\n",
    "columns_with_valid_names = [\n",
    "    col.replace(\" \", \"_\").replace(\"-\", \"_\").lower() for col in ngram_df.columns\n",
    "]\n",
    "ngram_df_renamed = ngram_df.toDF(*columns_with_valid_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array column to comma-separated string\n",
    "ngram_df_renamed = ngram_df_renamed.withColumn(\"ngrams\", concat_ws(\",\", col(\"ngrams\")))\n",
    "ngram_df_renamed = ngram_df_renamed.withColumn(\n",
    "    \"degrees\", concat_ws(\",\", col(\"degrees\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Posting Date\" to date format\n",
    "ngram_df_renamed = ngram_df_renamed.withColumn(\n",
    "    \"Posting_Date\", col(\"Posting_Date\").cast(\"date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_df_renamed.coalesce(1).write.mode(\"overwrite\").format(\"parquet\").save(\n",
    "    \"../../dataset/processed/nyc_job_postings_processed_data.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_df_renamed.coalesce(1).write.mode(\"overwrite\").format(\"csv\").option(\n",
    "    \"sep\", \"\\t\"\n",
    ").save(\"nyc_job_postings_processed_data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- job_id: integer (nullable = true)\n",
      " |-- agency: string (nullable = true)\n",
      " |-- posting_type: string (nullable = true)\n",
      " |-- #_of_positions: integer (nullable = true)\n",
      " |-- business_title: string (nullable = true)\n",
      " |-- job_category: string (nullable = true)\n",
      " |-- posting_date: timestamp (nullable = true)\n",
      " |-- annualsalaryfrom: double (nullable = true)\n",
      " |-- annualsalaryto: double (nullable = true)\n",
      " |-- averageannualsalary: double (nullable = true)\n",
      " |-- degrees: string (nullable = false)\n",
      " |-- highestdegree: string (nullable = true)\n",
      " |-- ngrams: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram_df_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x 1 root root 512 Aug 30 22:59 nyc_job_postings_processed_data.csv\r\n",
      "drwxr-xr-x 1 root root 512 Aug 30 22:35 nyc_job_postings_processed_data.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../../dataset/processed/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
