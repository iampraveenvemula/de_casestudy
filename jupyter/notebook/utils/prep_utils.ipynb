{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## contains cleaning and feature engineering modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    concat_ws,\n",
    "    lit,\n",
    "    lower,\n",
    "    regexp_replace,\n",
    "    lower,\n",
    "    split,\n",
    "    udf,\n",
    "    avg,\n",
    "    explode,\n",
    "    count,\n",
    "    when,\n",
    ")\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import NGram, StopWordsRemover\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from typing import List, Optional\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize NLTK lemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_annual_salary(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize salary range columns to annual salary.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input DataFrame containing salary information.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with standardized annual salary columns.\n",
    "    \"\"\"\n",
    "    workhours_per_day = 8\n",
    "    workdays_per_week = 5\n",
    "    workweeks_per_year = 52\n",
    "\n",
    "    workdays_per_year = workdays_per_week * workweeks_per_year\n",
    "    workhours_per_year = workhours_per_day * workdays_per_year\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"AnnualSalaryFrom\",\n",
    "        when(\n",
    "            col(\"Salary Frequency\") == \"Daily\",\n",
    "            col(\"Salary Range From\") * workdays_per_year,\n",
    "        )\n",
    "        .when(\n",
    "            col(\"Salary Frequency\") == \"Hourly\",\n",
    "            col(\"Salary Range From\") * workhours_per_year,\n",
    "        )\n",
    "        .otherwise(col(\"Salary Range From\")),\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"AnnualSalaryTo\",\n",
    "        when(\n",
    "            col(\"Salary Frequency\") == \"Daily\",\n",
    "            col(\"Salary Range To\") * workdays_per_year,\n",
    "        )\n",
    "        .when(\n",
    "            col(\"Salary Frequency\") == \"Hourly\",\n",
    "            col(\"Salary Range To\") * workhours_per_year,\n",
    "        )\n",
    "        .otherwise(col(\"Salary Range To\")),\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords for degrees\n",
    "keywords = [\n",
    "    \"master\",\n",
    "    \"phd\",\n",
    "    \"pg\",\n",
    "    \"post graduate\",\n",
    "    \"baccalaureate\",\n",
    "    \"diploma\",\n",
    "    \"high school\",\n",
    "]\n",
    "\n",
    "# Define the priority order for degrees\n",
    "degree_priority = [\n",
    "    \"phd\",\n",
    "    \"master\",\n",
    "    \"post graduate\",\n",
    "    \"pg\",\n",
    "    \"baccalaureate\",\n",
    "    \"diploma\",\n",
    "    \"high school\",\n",
    "]\n",
    "\n",
    "\n",
    "def build_regex(keywords: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Build a regular expression pattern to match keywords in a line.\n",
    "\n",
    "    Parameters:\n",
    "        keywords (list of str): List of keywords to build the regex pattern for.\n",
    "\n",
    "    Returns:\n",
    "        str: The regex pattern to match the keywords.\n",
    "    \"\"\"\n",
    "    res = \"(\"\n",
    "    for key in keywords:\n",
    "        res += \"\\\\b\" + key + \"\\\\b|\"\n",
    "    res = res[0 : len(res) - 1] + \")\"\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_matching_string(line: str, regex: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all matches of a regex pattern in a line.\n",
    "\n",
    "    Parameters:\n",
    "        line (str): The input line to search for matches.\n",
    "        regex (str): The regex pattern to search for.\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of matching strings found in the line.\n",
    "    \"\"\"\n",
    "    if line is None or regex is None:\n",
    "        return []  # Return an empty list for null inputs\n",
    "\n",
    "    matches = re.findall(regex, line)\n",
    "    return matches if matches else []\n",
    "\n",
    "\n",
    "def get_highest_degree(degrees: List[str]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get the highest priority degree from a list of degrees.\n",
    "\n",
    "    Parameters:\n",
    "        degrees (list of str): List of degrees to choose from.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The highest priority degree or None if no valid degree found.\n",
    "    \"\"\"\n",
    "\n",
    "    if degrees:\n",
    "        for degree in degree_priority:\n",
    "            if degree in degrees:\n",
    "                return degree\n",
    "    return None\n",
    "\n",
    "\n",
    "def safe_get_matching_string(line, regex):\n",
    "    if isinstance(line, (str, bytes)):\n",
    "        return get_matching_string(line, regex)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "get_degree_list_udf = udf(\n",
    "    lambda line, regex: safe_get_matching_string(line, regex), ArrayType(StringType())\n",
    ")\n",
    "\n",
    "get_highest_degree_udf = udf(get_highest_degree, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_column(\n",
    "    df: DataFrame, input_column: str, output_column: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess a text column by lowercasing and removing non-alphabet characters.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with text column.\n",
    "        input_column (str): Name of the input text column.\n",
    "        output_column (str): Name of the output column for cleaned text.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with the added cleaned text column.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_df = df.withColumn(\n",
    "        output_column, lower(regexp_replace(col(input_column), \"[^a-zA-Z\\s]\", \"\"))\n",
    "    )\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "def tokenize_and_remove_stopwords(\n",
    "    df: DataFrame, input_column: str, output_column: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Tokenize and remove stopwords from a text column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with text column.\n",
    "        input_column (str): Name of the input text column.\n",
    "        output_column (str): Name of the output column for tokenized and filtered words.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with tokenized and filtered words column.\n",
    "    \"\"\"\n",
    "    tokenized_df = df.withColumn(output_column, split(col(input_column), \"\\s+\"))\n",
    "\n",
    "    remover = StopWordsRemover(\n",
    "        inputCol=output_column, outputCol=\"filtered_\" + output_column\n",
    "    )\n",
    "    filtered_df = remover.transform(tokenized_df)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Define a UDF for lemmatization using NLTK\n",
    "def lemmatize_words(words):\n",
    "    if words is not None:\n",
    "        return [lemmatizer.lemmatize(word) for word in words]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def apply_lemmatization(\n",
    "    df: DataFrame, input_column: str, output_column: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply lemmatization to a text column using a provided lemmatization function.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with text column.\n",
    "        input_column (str): Name of the input text column.\n",
    "        output_column (str): Name of the output column for lemmatized words.\n",
    "        lemmatize_func: Lemmatization function to be applied.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with lemmatized words column.\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatize_udf = udf(lemmatize_words, ArrayType(StringType()))\n",
    "    lemmatized_df = df.withColumn(output_column, lemmatize_udf(col(input_column)))\n",
    "\n",
    "    return lemmatized_df\n",
    "\n",
    "\n",
    "def extract_ngrams(\n",
    "    df: DataFrame, input_column: str, output_column: str, n: int\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extract n-grams from a tokenized column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with tokenized words column.\n",
    "        input_column (str): Name of the input tokenized words column.\n",
    "        output_column (str): Name of the output column for extracted n-grams.\n",
    "        n (int): Number of n-grams.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with extracted n-grams column.\n",
    "    \"\"\"\n",
    "    ngram = NGram(n=n, inputCol=input_column, outputCol=output_column)\n",
    "    ngram_df = ngram.transform(df)\n",
    "\n",
    "    return ngram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
